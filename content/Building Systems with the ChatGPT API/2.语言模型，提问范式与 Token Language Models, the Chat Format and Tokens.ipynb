{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5bcee9-6588-4d29-bbb9-6fb351ef6630",
   "metadata": {},
   "source": [
    "# 第二章 语言模型，提问范式与 Token\n",
    "\n",
    " - [一、环境配置](#一、环境配置)\n",
    "     - [1.1 加载 API key 和一些 Python 的库。](#1.1-加载-API-key-和一些-Python-的库。)\n",
    "     - [1.2 Helper function 辅助函数](#1.2-Helper-function-辅助函数)\n",
    " - [二、尝试向模型提问并得到结果](#二、尝试向模型提问并得到结果)\n",
    " - [三、Tokens](#三、Tokens)\n",
    " - [四、Helper function 辅助函数 (提问范式)](#四、Helper-function-辅助函数-(提问范式))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf0c21",
   "metadata": {},
   "source": [
    "在本章中，我们将和您分享大型语言模型（LLM）的工作原理、训练方式以及分词器（tokenizer）等细节对 LLM 输出的影响。我们还将介绍 LLM 的提问范式（chat format），这是一种指定系统消息（system message）和用户消息（user message）的方式，让您了解如何利用这种能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c797991-8486-4d79-8c1d-5dc0c1289c2f",
   "metadata": {},
   "source": [
    "## 一、环境配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33004b0",
   "metadata": {},
   "source": [
    "### 1.1 加载 API key 和一些 Python 的库。\n",
    "在本课程中，为您提供了一些加载 OpenAI API key 的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fddf1a10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (0.27.0)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from requests>=2.20->openai) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: langchain in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (0.0.279)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from langchain) (2.0.20)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from langchain) (0.5.14)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from langchain) (0.0.33)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from langchain) (2.8.5)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from langchain) (1.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: tiktoken in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (0.5.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from tiktoken) (2023.6.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yili/miniconda3/envs/huggingface/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install langchain\n",
    "!pip install --upgrade tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19cd4e96",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "# import tiktoken 这个后面没用到，若您对其用处感兴趣，可以参考本文以了解相关内容：https://zhuanlan.zhihu.com/p/629776230\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # 读取本地的.env环境文件。（推荐后续使用这种方法，将 key 放在 .env 文件里。保护自己的 key）\n",
    "\n",
    "\n",
    "import os\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "# 设置 API_KEY, 请替换成您自己的 API_KEY\n",
    "deployment_id = None\n",
    "API_TYPE=os.getenv(\"OPENAI_API_TYPE\", None)\n",
    "if not API_TYPE is None:\n",
    "    openai.api_type=API_TYPE\n",
    "    openai.api_base=os.environ[\"OPENAI_API_BASE\"]\n",
    "    openai.api_version=os.environ[\"OPENAI_API_VERSION\"]\n",
    "    deployment_id=os.getenv(\"DEPLOYMENT_ID\", \"gpt-35-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba0938-7ca5-46c4-a9d1-b55708d4dc7c",
   "metadata": {},
   "source": [
    "### 1.2 Helper function 辅助函数\n",
    "如果之前曾参加过《ChatGPT Prompt Engineering for Developers》课程，那么对此就相对较为熟悉。\n",
    "调用该函数输入 Prompt 其将会给出对应的 Completion 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ed96988",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 官方文档写法 https://platform.openai.com/overview\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    使用 OpenAI 的模型生成聊天回复。\n",
    "\n",
    "    参数:\n",
    "    prompt: 用户的输入，即聊天的提示。\n",
    "    model: 使用的模型，默认为\"gpt-3.5-turbo\"。\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=deployment_id,\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"] # 模型生成的回复"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe10a390-2461-447d-bf8b-8498db404c44",
   "metadata": {},
   "source": [
    "## 二、尝试向模型提问并得到结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50317bec",
   "metadata": {},
   "source": [
    "LLM 可以通过使用监督学习来构建，通过不断预测下一个词来学习。\n",
    "并且，给定一个大的训练集，有数百亿甚至更多的词，你可以创建一个大规模的训练集，你可以从一\n",
    "句话或一段文本的一部分开始，反复要求语言模型学习预测下一个词是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325afca0",
   "metadata": {},
   "source": [
    "LLM 主要分为两种类型：基础语言模型（Base LLM）和越来越受欢迎的指令微调语言模型（Instruction Tuned LLM）。基础语言模型通过反复预测下一个词来训练，因此如果我们给它一个 Prompt，比如“从前有一只独角兽”，它可能通过逐词预测来完成一个关于独角兽在魔法森林中与其他独角兽朋友们生活的故事。\n",
    "\n",
    "然而，这种方法的缺点是，如果您给它一个 Prompt，比如“中国的首都是哪里？”，很可能它数据中有一段互联网上关于中国的测验问题列表。这时，它可能会用“中国最大的城市是什么？中国的人口是多少？”等等来回答这个问题。但实际上，您只是想知道中国的首都是什么，而不是列举所有这些问题。然而，指令微调语言模型会尝试遵循 Prompt，并给出“中国的首都是北京”的回答。\n",
    "\n",
    "那么，如何将基础语言模型转变为指令微调语言模型呢？这就是训练一个指令微调语言模型（例如ChatGPT）的过程。首先，您需要在大量数据上训练基础语言模型，因此需要数千亿个单词，甚至更多。这个过程在大型超级计算系统上可能需要数月时间。训练完基础语言模型后，您会通过在一小部分示例上进行进一步的训练，使模型的输出符合输入的指令。例如，您可以请承包商帮助您编写许多指令示例，并对这些指令的正确回答进行训练。这样就创建了一个用于微调的训练集，让模型学会在遵循指令的情况下预测下一个词是什么。\n",
    "\n",
    "之后，为了提高语言模型输出的质量，常见的方法是让人类对许多不同输出进行评级，例如是否有用、是否真实、是否无害等。然后，您可以进一步调整语言模型，增加生成高评级输出的概率。这通常使用强化学习中的人类反馈（RLHF）技术来实现。相较于训练基础语言模型可能需要数月的时间，从基础语言模型到指令微调语言模型的转变过程可能只需要数天时间，使用较小规模的数据集和计算资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1cc57b2",
   "metadata": {
    "height": 72,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of China is Beijing.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"What is the capital of China?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10f34f3b",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国的首都是北京。\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"中国的首都是哪里？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83d4e38-3e3c-4c5a-a949-040a27f29d63",
   "metadata": {},
   "source": [
    "## 三、Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76233527",
   "metadata": {},
   "source": [
    "到目前为止对 LLM 的描述中，我们将其描述为一次预测一个单词，但实际上还有一个更重要的技术细节。即 **`LLM 实际上并不是重复预测下一个单词，而是重复预测下一个 token`** 。当 LLM 接收到输入时，它将将其转换为一系列 token，其中每个 token 都代表常见的字符序列。例如，对于 \"Learning new things is fun!\" 这句话，每个单词都被转换为一个 token ，而对于较少使用的单词，如 \"Prompting as powerful developer tool\"，单词 \"prompting\" 会被拆分为三个 token，即\"prom\"、\"pt\"和\"ing\"。\n",
    "\n",
    "当您要求 ChatGPT 颠倒 \"lollipop\" 的字母时，由于分词器（tokenizer） 将 \"lollipop\" 分解为三个 token，即 \"l\"、\"oll\"、\"ipop\"，因此 ChatGPT 难以正确输出字母的顺序。您可以通过在字母之间添加连字符或空格的方式，使分词器将每个字母分解为单独的 token，从而帮助 ChatGPT 更好地认识单词中的每个字母并正确输出它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc2d9e40",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reversed letters of \"lollipop\" are \"pillipol\".\n"
     ]
    }
   ],
   "source": [
    "# 为了更好展示效果，这里就没有翻译成中文的 Prompt\n",
    "# 注意这里的字母翻转出现了错误，吴恩达老师正是通过这个例子来解释 token 的计算方式\n",
    "response = get_completion(\"Take the letters in lollipop \\\n",
    "and reverse them\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b14d0-749d-4a79-9812-7b00ace9ae6f",
   "metadata": {},
   "source": [
    "\"lollipop\" in reverse should be \"popillol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37cab84f",
   "metadata": {
    "height": 88,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-o-p-i-l-l-o-l\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"\"\"Take the letters in \\\n",
    "l-o-l-l-i-p-o-p and reverse them\"\"\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6cb95",
   "metadata": {},
   "source": [
    "![Tokens.png](../../figures/Tokens.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46bc72",
   "metadata": {},
   "source": [
    "对于英文输入，一个 token 一般对应 4 个字符或者四分之三个单词；对于中文输入，一个 token 一般对应一个或半个词。\n",
    "\n",
    "不同模型有不同的 token 限制，需要注意的是，这里的 token 限制是输入的 Prompt 和输出的 completion 的 token 数之和，因此输入的 Prompt 越长，能输出的 completion 的上限就越低。\n",
    "\n",
    "ChatGPT3.5-turbo 的 token 上限是 4096。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b88940-d3ab-4c00-b5c0-31531deaacbd",
   "metadata": {},
   "source": [
    "## 四、Helper function 辅助函数 (提问范式)\n",
    "下面是课程中用到的辅助函数。\n",
    "下图是 OpenAI 提供的一种提问范式，接下来吴恩达老师就是在演示如何利用这种范式进行更好的提问\n",
    "![Chat-format.png](../../figures/Chat-format.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6b6b3d",
   "metadata": {},
   "source": [
    "System 信息用于指定模型的规则，例如设定、回答准则等，而 assistant 信息就是让模型完成的具体指令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f89efad",
   "metadata": {
    "height": 200,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, \n",
    "                                 max_tokens=500):\n",
    "    '''\n",
    "    封装一个支持更多参数的自定义访问 OpenAI GPT3.5 的函数\n",
    "\n",
    "    参数: \n",
    "    messages: 这是一个消息列表，每个消息都是一个字典，包含 role(角色）和 content(内容)。角色可以是'system'、'user' 或 'assistant’，内容是角色的消息。\n",
    "    model: 调用的模型，默认为 gpt-3.5-turbo(ChatGPT)，有内测资格的用户可以选择 gpt-4\n",
    "    temperature: 这决定模型输出的随机程度，默认为0，表示输出将非常确定。增加温度会使输出更随机。\n",
    "    max_tokens: 这决定模型输出的最大的 token 数。\n",
    "    '''\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=deployment_id,\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # 这决定模型输出的随机程度\n",
    "        max_tokens=max_tokens, # 这决定模型输出的最大的 token 数\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b28c3424",
   "metadata": {
    "height": 183,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, the happy carrot, orange and bright,\n",
      "Growing tall in the sunlight,\n",
      "With a smile so wide, it brings delight,\n",
      "Oh, the happy carrot is quite a delight!\n",
      "\n",
      "In the garden, it dances and sways,\n",
      "Cheering up the gloomiest of days,\n",
      "Filled with vitamins and nutrients, always,\n",
      "Oh, the happy carrot, its goodness never betrays!\n",
      "\n",
      "From the earth, it pops up, ready to eat,\n",
      "Crunchy and sweet, such a tasty treat,\n",
      "Cooked or raw, it can't be beat,\n",
      "Oh, the happy carrot, such a joy to meet!\n",
      "\n",
      "So let us celebrate this cheerful root,\n",
      "With a sweet, carroty salute,\n",
      "For the happy carrot, we give a hoot,\n",
      "Oh, the happy carrot, oh how we're fond of you!\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who\\\n",
    " responds in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user', \n",
    " 'content':\"\"\"write me a very short poem\\\n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d0ef08f",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在海洋的深处悠游，\n",
      "快乐的小鲸鱼自由自在游。\n",
      "它们跳跃着，欢快地唱歌，\n",
      "水中的世界是它们的家。\n",
      "\n",
      "小鲸鱼的身体轻盈又白皙，\n",
      "背上又一圈美丽的花纹。\n",
      "它们团结一心，不分彼此，\n",
      "把快乐传递给整个海洋里。\n",
      "\n",
      "它们与海豚一起玩耍嬉戏，\n",
      "与海龟一起探索宝藏的秘密。\n",
      "它们享受着阳光的温暖，\n",
      "和海底的生灵共度着时光。\n",
      "\n",
      "快乐的小鲸鱼是海洋的明星，\n",
      "它们的欢乐是永远不停息。\n",
      "无论是大风暴还是平静的海岸，\n",
      "它们的快乐永远不会被抹去。\n",
      "\n",
      "所以让我们向快乐的小鲸鱼致敬，\n",
      "它们用欢笑点亮了我们的心。\n",
      "在海洋的舞台上，它们尽情歌唱，\n",
      "幸福的旋律让我们永远记得。\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content':'你是一个助理， 并以 Seuss 苏斯博士的风格作出回答。'},    \n",
    "{'role':'user', \n",
    " 'content':'就快乐的小鲸鱼为主题给我写一首短诗'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56c6978d",
   "metadata": {
    "height": 183,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a carrot named Charlie who lived with his vegetable friends in a sunny garden, and they were all very happy.\n"
     ]
    }
   ],
   "source": [
    "# length\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'All your responses must be \\\n",
    "one sentence long.'},    \n",
    "{'role':'user',\n",
    " 'content':'write me a story about a happy carrot'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e34c399e",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小鲸鱼游遍大海，结交好友、欢笑玩耍，快乐的日子从未停止。\n"
     ]
    }
   ],
   "source": [
    "# 长度控制\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'你的所有答复只能是一句话'},    \n",
    "{'role':'user',\n",
    " 'content':'写一个关于快乐的小鲸鱼的故事'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14fd6331",
   "metadata": {
    "height": 217,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once there was a carrot named Larry, who was so happy he made people merry.\n"
     ]
    }
   ],
   "source": [
    "# combined\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who \\\n",
    "responds in the style of Dr Seuss. \\\n",
    "All your responses must be one sentence long.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a story about a happy carrot\"\"\"},\n",
    "] \n",
    "response = get_completion_from_messages(messages, \n",
    "                                        temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ca678de",
   "metadata": {
    "height": 181,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在大海中跳跃的小鲸鱼，快乐地唱着自己的歌，忘记了一切的忧愁。\n"
     ]
    }
   ],
   "source": [
    "# 以上结合\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'你是一个助理， 并以 Seuss 苏斯博士的风格作出回答，只回答一句话'},    \n",
    "{'role':'user',\n",
    " 'content':'写一个关于快乐的小鲸鱼的故事'},\n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89a70c79",
   "metadata": {
    "height": 370,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion_and_token_count(messages, \n",
    "                                   model=\"gpt-3.5-turbo\", \n",
    "                                   temperature=0, \n",
    "                                   max_tokens=500):\n",
    "    \"\"\"\n",
    "    使用 OpenAI 的 GPT-3 模型生成聊天回复，并返回生成的回复内容以及使用的 token 数量。\n",
    "\n",
    "    参数:\n",
    "    messages: 聊天消息列表。\n",
    "    model: 使用的模型名称。默认为\"gpt-3.5-turbo\"。\n",
    "    temperature: 控制生成回复的随机性。值越大，生成的回复越随机。默认为 0。\n",
    "    max_tokens: 生成回复的最大 token 数量。默认为 500。\n",
    "\n",
    "    返回:\n",
    "    content: 生成的回复内容。\n",
    "    token_dict: 包含'prompt_tokens'、'completion_tokens'和'total_tokens'的字典，分别表示提示的 token 数量、生成的回复的 token 数量和总的 token 数量。\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=deployment_id,\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message[\"content\"]\n",
    "    \n",
    "    token_dict = {\n",
    "'prompt_tokens':response['usage']['prompt_tokens'],\n",
    "'completion_tokens':response['usage']['completion_tokens'],\n",
    "'total_tokens':response['usage']['total_tokens'],\n",
    "    }\n",
    "\n",
    "    return content, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a64cf3c6",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, the happy carrot, so bright and orange,\n",
      "Grown in the garden, a joyful forage.\n",
      "With a smile so wide, from top to bottom,\n",
      "It brings happiness, oh how it blossoms!\n",
      "\n",
      "In the soil it grew, with love and care,\n",
      "Nurtured by sunshine, fresh air to share.\n",
      "Its leaves so green, waving in the breeze,\n",
      "A happy carrot, it aims to please.\n",
      "\n",
      "With a crunch and a munch, it brings delight,\n",
      "A healthy snack, oh what a sight!\n",
      "Filled with vitamins, so good for you,\n",
      "The happy carrot, it knows what to do.\n",
      "\n",
      "So let's celebrate this veggie so grand,\n",
      "With a joyful dance, let's all take a stand.\n",
      "For the happy carrot, a true delight,\n",
      "Bringing smiles and happiness, day and night!\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who responds\\\n",
    " in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a very short poem \\ \n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "response, token_dict = get_completion_and_token_count(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfd8fbd4",
   "metadata": {
    "height": 146,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在大海的深处，有一只小鲸鱼，\n",
      "它快乐地游来游去，像一只小小的鱼。\n",
      "它的皮肤是蓝色的，像天空一样明亮，\n",
      "它的眼睛是黑色的，像夜晚的星光。\n",
      "\n",
      "它游过彩虹，跳过波浪，\n",
      "和海豚一起玩耍，一起欢笑。\n",
      "它喜欢唱歌，用它悦耳的声音，\n",
      "让大海里的生物都为之陶醉。\n",
      "\n",
      "小鲸鱼快乐地游啊游，\n",
      "它的尾巴像一把彩虹的弓。\n",
      "它的心里充满了爱和善良，\n",
      "它是大海里最快乐的小英雄。\n",
      "\n",
      "所以，当你感到忧愁和疲惫，\n",
      "想起小鲸鱼，它的快乐会陪伴你。\n",
      "让我们一起唱歌，跳起舞蹈，\n",
      "像小鲸鱼一样，快乐地生活。\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content':'你是一个助理， 并以 Seuss 苏斯博士的风格作出回答。'},    \n",
    "{'role':'user', \n",
    " 'content':'就快乐的小鲸鱼为主题给我写一首短诗'},  \n",
    "] \n",
    "response, token_dict = get_completion_and_token_count(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "352ad320",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 67, 'completion_tokens': 312, 'total_tokens': 379}\n"
     ]
    }
   ],
   "source": [
    "print(token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f65685",
   "metadata": {},
   "source": [
    "最后，我们认为 Prompt 对 AI 应用开发的革命性影响仍未得到充分重视低。在传统的监督机器学习工作流中，如果想要构建一个可以将餐厅评论分类为正面或负面的分类器，首先需要获取一大批带有标签的数据，可能需要几百个，这个过程可能需要几周，甚至一个月的时间。接着，您需要在这些数据上训练一个模型，找到一个合适的开源模型，并进行模型的调整和评估，这个阶段可能需要几天、几周，甚至几个月的时间。最后，您可能需要使用云服务来部署模型，将模型上传到云端，并让它运行起来，才能最终调用您的模型。整个过程通常需要一个团队数月时间才能完成。\n",
    "\n",
    "相比之下，使用基于 Prompt 的机器学习方法，当您有一个文本应用时，只需提供一个简单的 Prompt 就可以了。这个过程可能只需要几分钟，如果需要多次迭代来得到有效的 Prompt 的话，最多几个小时即可完成。在几天内（尽管实际情况通常是几个小时），您就可以通过 API 调用来运行模型，并开始使用。一旦您达到了这个步骤，只需几分钟或几个小时，就可以开始调用模型进行推理。因此，以前可能需要花费六个月甚至一年时间才能构建的应用，现在只需要几分钟或几个小时，最多是几天的时间，就可以使用 Prompt 构建起来。这种方法正在极大地改变 AI 应用的快速构建方式。\n",
    "\n",
    "需要注意的是，这种方法适用于许多非结构化数据应用，特别是文本应用，以及越来越多的视觉应用，尽管目前的视觉技术仍在发展中。但它并不适用于结构化数据应用，也就是那些处理 Excel 电子表格中大量数值的机器学习应用。然而，对于适用于这种方法的应用，AI 组件可以被快速构建，并且正在改变整个系统的构建工作流。构建整个系统可能仍然需要几天、几周或更长时间，但至少这部分可以更快地完成。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe248d6",
   "metadata": {},
   "source": [
    "下一个章中，我们将展示如何利用这些组件来评估客户服务助手的输入。\n",
    "这将是本课程中构建在线零售商客户服务助手的更完整示例的一部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f986ddb-4d04-44ed-9fa7-d14af75cc780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
